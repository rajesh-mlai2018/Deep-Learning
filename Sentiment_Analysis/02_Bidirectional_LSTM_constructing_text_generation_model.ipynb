{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ph5eir3Pf-3z"
   },
   "source": [
    "# Constructing a Text Generation Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GbGfr_oLCat"
   },
   "source": [
    "It's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aHK2CYygXom"
   },
   "source": [
    "## Import TensorFlow and related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2LmLTREBf5ng"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Other imports for processing data\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import glob as glob\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmLTO_dpgge9"
   },
   "source": [
    "## Get the Dataset\n",
    "\n",
    "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Bf5FVHfganK",
    "outputId": "726cd9c5-cb72-4989-e15f-1fb631d9c21a"
   },
   "source": [
    "!wget --no-check-certificate \\\n",
    "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
    "    -O /tmp/songdata.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gu1BTzMIS1oy"
   },
   "source": [
    "## **First 10 Songs**\n",
    "\n",
    "Let's first look at just 10 songs from the dataset, and see how things perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmb9rGaAUDO-"
   },
   "source": [
    "### Preprocessing\n",
    "\n",
    "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "2AVAvyF_Vuh5"
   },
   "outputs": [],
   "source": [
    "def tokenize_corpus(corpus, num_words=-1):\n",
    "    # Fit a Tokenizer on the corpus\n",
    "    if num_words > -1:\n",
    "        tokenizer = Tokenizer(num_words=num_words)\n",
    "        else:\n",
    "            tokenizer = Tokenizer()\n",
    "            tokenizer.fit_on_texts(corpus)\n",
    "    return tokenizer\n",
    "\n",
    "def create_lyrics_corpus(dataset, field):\n",
    "    # Remove all other punctuation\n",
    "    dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
    "    # Make it lowercase\n",
    "    dataset[field] = dataset[field].str.lower()\n",
    "    # Make it one long string to split by line\n",
    "    lyrics = dataset[field].str.cat()\n",
    "    corpus = lyrics.split('\\n')\n",
    "    # Remove any trailing whitespace\n",
    "    for l in range(len(corpus)):\n",
    "        corpus[l] = corpus[l].rstrip()\n",
    "    # Remove any empty lines\n",
    "    corpus = [l for l in corpus if l != '']\n",
    "  \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_input_files=['songdata']\n",
    "l_datapath=os.path.join(os.getcwd(),'data')\n",
    "    \n",
    "l_filepath =[j for j in glob.glob(os.path.join(l_datapath,'*')) if os.path.basename(j) ==l_input_files[0]+'.csv' ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file songdata.csv\n",
      "Dataframe songdata_df created\n"
     ]
    }
   ],
   "source": [
    "for n in l_input_files:\n",
    "    for f in l_filepath:\n",
    "        if n==Path(f).stem :\n",
    "            print('reading file {}'.format(os.path.basename(f)))\n",
    "            vars()[n+'_df']=pd.read_csv(f)\n",
    "            print('Dataframe {} created'.format(n+'_df'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset from csv - just first 10 songs for now\n",
    "#dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
    "songdata_df1=songdata_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apcEXp7WhVBs",
    "outputId": "9074d4f5-5c8b-4325-8a5c-16395e62a6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
      "495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the corpus using the 'text' column containing lyrics\n",
    "corpus = create_lyrics_corpus(dataset, 'text')\n",
    "# Tokenize the corpus\n",
    "tokenizer = tokenize_corpus(corpus)\n",
    "\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "print(tokenizer.word_index)\n",
    "print(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9x68iN_X6FK"
   },
   "source": [
    "### Create Sequences and Labels\n",
    "\n",
    "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QmlTsUqfikVO"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        sequences.append(n_gram_sequence)\n",
    "\n",
    "# Pad sequences for equal input length \n",
    "max_sequence_len = max([len(seq) for seq in sequences])\n",
    "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
    "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
    "# One-hot encode the labels\n",
    "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zsmu3aEId49i",
    "outputId": "34c3645e-c841-463b-d27a-0b44623b94bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "97\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
      "   4]\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
      " 287]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Check out how some of our data is being stored\n",
    "# The Tokenizer has just a single index per word\n",
    "print(tokenizer.word_index['know'])\n",
    "print(tokenizer.word_index['feeling'])\n",
    "# Input sequences will have multiple indexes\n",
    "print(input_sequences[5])\n",
    "print(input_sequences[6])\n",
    "# And the one hot labels will be as long as the full spread of tokenized words\n",
    "print(one_hot_labels[5])\n",
    "print(one_hot_labels[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1TAJMlmfO8r"
   },
   "source": [
    "### Train a Text Generation Model\n",
    "\n",
    "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
    "\n",
    "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1YXuxIqfygN",
    "outputId": "72ef3cdf-1ece-4850-e3c3-f470a825a6c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "62/62 [==============================] - 5s 7ms/step - loss: 5.9577 - accuracy: 0.0378\n",
      "Epoch 2/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.4347 - accuracy: 0.0399\n",
      "Epoch 3/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.3747 - accuracy: 0.0399\n",
      "Epoch 4/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.3225 - accuracy: 0.0399\n",
      "Epoch 5/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.2540 - accuracy: 0.0388\n",
      "Epoch 6/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.1833 - accuracy: 0.0409\n",
      "Epoch 7/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.1189 - accuracy: 0.0464\n",
      "Epoch 8/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 5.0589 - accuracy: 0.0444\n",
      "Epoch 9/200\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 4.9980 - accuracy: 0.0469\n",
      "Epoch 10/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.9261 - accuracy: 0.0520\n",
      "Epoch 11/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.8468 - accuracy: 0.0691\n",
      "Epoch 12/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.7611 - accuracy: 0.0782\n",
      "Epoch 13/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.6806 - accuracy: 0.0762\n",
      "Epoch 14/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.5995 - accuracy: 0.0843\n",
      "Epoch 15/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.5249 - accuracy: 0.0918\n",
      "Epoch 16/200\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 4.4474 - accuracy: 0.0959\n",
      "Epoch 17/200\n",
      "62/62 [==============================] - 1s 10ms/step - loss: 4.3778 - accuracy: 0.1024\n",
      "Epoch 18/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.2996 - accuracy: 0.1095\n",
      "Epoch 19/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 4.2357 - accuracy: 0.1186\n",
      "Epoch 20/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 4.1566 - accuracy: 0.1377\n",
      "Epoch 21/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 4.0902 - accuracy: 0.1493\n",
      "Epoch 22/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 4.0333 - accuracy: 0.1493\n",
      "Epoch 23/200\n",
      "62/62 [==============================] - 1s 12ms/step - loss: 3.9628 - accuracy: 0.1690\n",
      "Epoch 24/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.9119 - accuracy: 0.1852\n",
      "Epoch 25/200\n",
      "62/62 [==============================] - 1s 11ms/step - loss: 3.8537 - accuracy: 0.1837\n",
      "Epoch 26/200\n",
      "62/62 [==============================] - 1s 9ms/step - loss: 3.7997 - accuracy: 0.1978\n",
      "Epoch 27/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.7253 - accuracy: 0.2084\n",
      "Epoch 28/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.6795 - accuracy: 0.2099\n",
      "Epoch 29/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.6142 - accuracy: 0.2301\n",
      "Epoch 30/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.5518 - accuracy: 0.2477\n",
      "Epoch 31/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.4952 - accuracy: 0.2694\n",
      "Epoch 32/200\n",
      "62/62 [==============================] - 1s 8ms/step - loss: 3.4357 - accuracy: 0.2745\n",
      "Epoch 33/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.3809 - accuracy: 0.2936\n",
      "Epoch 34/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.3503 - accuracy: 0.3078\n",
      "Epoch 35/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.2808 - accuracy: 0.3224\n",
      "Epoch 36/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.2283 - accuracy: 0.3355\n",
      "Epoch 37/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.1680 - accuracy: 0.3547\n",
      "Epoch 38/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 3.1214 - accuracy: 0.3597\n",
      "Epoch 39/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.0751 - accuracy: 0.3663\n",
      "Epoch 40/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 3.0438 - accuracy: 0.3769\n",
      "Epoch 41/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.9999 - accuracy: 0.3870\n",
      "Epoch 42/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.9422 - accuracy: 0.3930\n",
      "Epoch 43/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.9029 - accuracy: 0.4057\n",
      "Epoch 44/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 2.8542 - accuracy: 0.4051\n",
      "Epoch 45/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.7992 - accuracy: 0.4188\n",
      "Epoch 46/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.7487 - accuracy: 0.4309\n",
      "Epoch 47/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.7243 - accuracy: 0.4324\n",
      "Epoch 48/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 2.7443 - accuracy: 0.4314\n",
      "Epoch 49/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.6536 - accuracy: 0.4460\n",
      "Epoch 50/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.6030 - accuracy: 0.4561\n",
      "Epoch 51/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.5481 - accuracy: 0.4682\n",
      "Epoch 52/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.5191 - accuracy: 0.4753\n",
      "Epoch 53/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.4864 - accuracy: 0.4783\n",
      "Epoch 54/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.4521 - accuracy: 0.4854\n",
      "Epoch 55/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.4099 - accuracy: 0.4919\n",
      "Epoch 56/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.3653 - accuracy: 0.5071\n",
      "Epoch 57/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.3220 - accuracy: 0.5106\n",
      "Epoch 58/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.3261 - accuracy: 0.5146\n",
      "Epoch 59/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.4061 - accuracy: 0.4859\n",
      "Epoch 60/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.3561 - accuracy: 0.5005\n",
      "Epoch 61/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.2555 - accuracy: 0.5222\n",
      "Epoch 62/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.1909 - accuracy: 0.5394\n",
      "Epoch 63/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.3667 - accuracy: 0.4990\n",
      "Epoch 64/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.3086 - accuracy: 0.5156\n",
      "Epoch 65/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.2133 - accuracy: 0.5353\n",
      "Epoch 66/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.1551 - accuracy: 0.5454\n",
      "Epoch 67/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.0976 - accuracy: 0.5515\n",
      "Epoch 68/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.0476 - accuracy: 0.5721\n",
      "Epoch 69/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 2.0023 - accuracy: 0.5843\n",
      "Epoch 70/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.9582 - accuracy: 0.5964\n",
      "Epoch 71/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.9264 - accuracy: 0.6070\n",
      "Epoch 72/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.8962 - accuracy: 0.6024\n",
      "Epoch 73/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.8586 - accuracy: 0.6135\n",
      "Epoch 74/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.8364 - accuracy: 0.6191\n",
      "Epoch 75/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.8014 - accuracy: 0.6297\n",
      "Epoch 76/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.7588 - accuracy: 0.6443\n",
      "Epoch 77/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.7267 - accuracy: 0.6478\n",
      "Epoch 78/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.6968 - accuracy: 0.6529\n",
      "Epoch 79/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.6721 - accuracy: 0.6589\n",
      "Epoch 80/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.6655 - accuracy: 0.6609\n",
      "Epoch 81/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.6351 - accuracy: 0.6715\n",
      "Epoch 82/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.6050 - accuracy: 0.6736\n",
      "Epoch 83/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.5713 - accuracy: 0.6761\n",
      "Epoch 84/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.5504 - accuracy: 0.6887\n",
      "Epoch 85/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.5248 - accuracy: 0.6821\n",
      "Epoch 86/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.4997 - accuracy: 0.6932\n",
      "Epoch 87/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.4700 - accuracy: 0.6968\n",
      "Epoch 88/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.4513 - accuracy: 0.7033\n",
      "Epoch 89/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 1.4364 - accuracy: 0.6968\n",
      "Epoch 90/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 1.4160 - accuracy: 0.7008\n",
      "Epoch 91/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3925 - accuracy: 0.7084\n",
      "Epoch 92/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3691 - accuracy: 0.7195\n",
      "Epoch 93/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3568 - accuracy: 0.7094\n",
      "Epoch 94/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3466 - accuracy: 0.7129\n",
      "Epoch 95/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3316 - accuracy: 0.7164\n",
      "Epoch 96/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3128 - accuracy: 0.7205\n",
      "Epoch 97/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3147 - accuracy: 0.7210\n",
      "Epoch 98/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3254 - accuracy: 0.7159\n",
      "Epoch 99/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.3026 - accuracy: 0.7260\n",
      "Epoch 100/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.2991 - accuracy: 0.7190\n",
      "Epoch 101/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.2494 - accuracy: 0.7381\n",
      "Epoch 102/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.2403 - accuracy: 0.7255\n",
      "Epoch 103/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.2094 - accuracy: 0.7437\n",
      "Epoch 104/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.1954 - accuracy: 0.7487\n",
      "Epoch 105/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.1703 - accuracy: 0.7472\n",
      "Epoch 106/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.1657 - accuracy: 0.7513\n",
      "Epoch 107/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.1380 - accuracy: 0.7619\n",
      "Epoch 108/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.1248 - accuracy: 0.7553\n",
      "Epoch 109/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.1003 - accuracy: 0.7699\n",
      "Epoch 110/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.0890 - accuracy: 0.7674\n",
      "Epoch 111/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.0712 - accuracy: 0.7704\n",
      "Epoch 112/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.0616 - accuracy: 0.7725\n",
      "Epoch 113/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.0550 - accuracy: 0.7740\n",
      "Epoch 114/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.0393 - accuracy: 0.7790\n",
      "Epoch 115/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.0203 - accuracy: 0.7740\n",
      "Epoch 116/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 1.0010 - accuracy: 0.7851\n",
      "Epoch 117/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9891 - accuracy: 0.7846\n",
      "Epoch 118/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9770 - accuracy: 0.7952\n",
      "Epoch 119/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9649 - accuracy: 0.7916\n",
      "Epoch 120/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9552 - accuracy: 0.7931\n",
      "Epoch 121/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9516 - accuracy: 0.7896\n",
      "Epoch 122/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9425 - accuracy: 0.7921\n",
      "Epoch 123/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9285 - accuracy: 0.7952\n",
      "Epoch 124/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9155 - accuracy: 0.7977\n",
      "Epoch 125/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.9116 - accuracy: 0.7911\n",
      "Epoch 126/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8974 - accuracy: 0.7957\n",
      "Epoch 127/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8965 - accuracy: 0.7967\n",
      "Epoch 128/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8816 - accuracy: 0.8047\n",
      "Epoch 129/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8660 - accuracy: 0.8027\n",
      "Epoch 130/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8494 - accuracy: 0.8093\n",
      "Epoch 131/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8586 - accuracy: 0.8063\n",
      "Epoch 132/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8505 - accuracy: 0.8037\n",
      "Epoch 133/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8352 - accuracy: 0.8098\n",
      "Epoch 134/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8252 - accuracy: 0.8148\n",
      "Epoch 135/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 0.8094 - accuracy: 0.8209\n",
      "Epoch 136/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7964 - accuracy: 0.8158\n",
      "Epoch 137/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7965 - accuracy: 0.8209\n",
      "Epoch 138/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7957 - accuracy: 0.8224\n",
      "Epoch 139/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.8236 - accuracy: 0.8083\n",
      "Epoch 140/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7919 - accuracy: 0.8194\n",
      "Epoch 141/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7721 - accuracy: 0.8249\n",
      "Epoch 142/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7677 - accuracy: 0.8264\n",
      "Epoch 143/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7602 - accuracy: 0.8274\n",
      "Epoch 144/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7473 - accuracy: 0.8290\n",
      "Epoch 145/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7323 - accuracy: 0.8330\n",
      "Epoch 146/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7267 - accuracy: 0.8330\n",
      "Epoch 147/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7115 - accuracy: 0.8365\n",
      "Epoch 148/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7033 - accuracy: 0.8416\n",
      "Epoch 149/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7484 - accuracy: 0.8199\n",
      "Epoch 150/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7867 - accuracy: 0.8078\n",
      "Epoch 151/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7496 - accuracy: 0.8285\n",
      "Epoch 152/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7366 - accuracy: 0.8295\n",
      "Epoch 153/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7263 - accuracy: 0.8350\n",
      "Epoch 154/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.7033 - accuracy: 0.8396\n",
      "Epoch 155/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6933 - accuracy: 0.8411\n",
      "Epoch 156/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6771 - accuracy: 0.8406\n",
      "Epoch 157/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 0.6757 - accuracy: 0.8461\n",
      "Epoch 158/200\n",
      "62/62 [==============================] - 0s 8ms/step - loss: 0.6637 - accuracy: 0.8572\n",
      "Epoch 159/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6507 - accuracy: 0.8486\n",
      "Epoch 160/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6457 - accuracy: 0.8542\n",
      "Epoch 161/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6328 - accuracy: 0.8547\n",
      "Epoch 162/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6227 - accuracy: 0.8633\n",
      "Epoch 163/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6154 - accuracy: 0.8597\n",
      "Epoch 164/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6075 - accuracy: 0.8618\n",
      "Epoch 165/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6042 - accuracy: 0.8648\n",
      "Epoch 166/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6192 - accuracy: 0.8577\n",
      "Epoch 167/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6517 - accuracy: 0.8436\n",
      "Epoch 168/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6326 - accuracy: 0.8532\n",
      "Epoch 169/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6259 - accuracy: 0.8532\n",
      "Epoch 170/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.6073 - accuracy: 0.8597\n",
      "Epoch 171/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5848 - accuracy: 0.8648\n",
      "Epoch 172/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5724 - accuracy: 0.8643\n",
      "Epoch 173/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5619 - accuracy: 0.8673\n",
      "Epoch 174/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5575 - accuracy: 0.8688\n",
      "Epoch 175/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5459 - accuracy: 0.8703\n",
      "Epoch 176/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5405 - accuracy: 0.8744\n",
      "Epoch 177/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5355 - accuracy: 0.8749\n",
      "Epoch 178/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5320 - accuracy: 0.8764\n",
      "Epoch 179/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5254 - accuracy: 0.8744\n",
      "Epoch 180/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5207 - accuracy: 0.8764\n",
      "Epoch 181/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5173 - accuracy: 0.8744\n",
      "Epoch 182/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5122 - accuracy: 0.8749\n",
      "Epoch 183/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5077 - accuracy: 0.8794\n",
      "Epoch 184/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5084 - accuracy: 0.8779\n",
      "Epoch 185/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5621 - accuracy: 0.8592\n",
      "Epoch 186/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5620 - accuracy: 0.8592\n",
      "Epoch 187/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5605 - accuracy: 0.8602\n",
      "Epoch 188/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5475 - accuracy: 0.8638\n",
      "Epoch 189/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5618 - accuracy: 0.8613\n",
      "Epoch 190/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5153 - accuracy: 0.8693\n",
      "Epoch 191/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5022 - accuracy: 0.8754\n",
      "Epoch 192/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4905 - accuracy: 0.8764\n",
      "Epoch 193/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4838 - accuracy: 0.8739\n",
      "Epoch 194/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4752 - accuracy: 0.8794\n",
      "Epoch 195/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4751 - accuracy: 0.8779\n",
      "Epoch 196/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4670 - accuracy: 0.8754\n",
      "Epoch 197/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4899 - accuracy: 0.8739\n",
      "Epoch 198/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.5034 - accuracy: 0.8769\n",
      "Epoch 199/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4707 - accuracy: 0.8824\n",
      "Epoch 200/200\n",
      "62/62 [==============================] - 0s 7ms/step - loss: 0.4630 - accuracy: 0.8819\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(20)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXVFpoREhV6Y"
   },
   "source": [
    "### View the Training Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "aeSNfS7uhch0",
    "outputId": "7cecfb8e-12bf-4de4-ddd7-d70603cae571"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.show()\n",
    "\n",
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rAgRpxYhjpB"
   },
   "source": [
    "### Generate new lyrics!\n",
    "\n",
    "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DC7zfcgviDTp",
    "outputId": "b17b13bc-239c-4521-93a1-cacdd4bd903c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im feeling chills me time and time me they go and the scars theyre leaving cause i do what crazy wanted new crazy would would scars night night night night hope new oh world do new hope world new hope do what new hope new oh world wanted walk broken cassandra a new break break tool know night baby world new new notion tune world tune us wanted never world am cassandra cassandra had had had always love youd world could could i had to house always break caught above always break touch shoulder would know making world came feeling best heartaches night\n"
     ]
    }
   ],
   "source": [
    "seed_text = \"im feeling chills\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
